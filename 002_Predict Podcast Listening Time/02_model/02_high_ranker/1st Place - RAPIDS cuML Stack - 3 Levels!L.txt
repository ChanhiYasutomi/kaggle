1st Place - RAPIDS cuML Stack - 3 Levels!

url:https://www.kaggle.com/competitions/playground-series-s5e4/discussion/575784

Thank you Kaggle for a great playground competition. This month's playground competition has a great dataset with lots of interesting patterns within! (and it felt more like real data and less like synthetic data). And there are lots of strong competitors which made this competition fun and exciting!

The target Listening_Time_minutes is approximately equal to the linear relationship 0.72 x Episode_Length_minutes as described in my 3 discussion posts here:①, here:②, here:③. The other 9 features modulate this linear relationship. Based on this insight, I stacked the following approaches:



My favorite solution for a Kaggle competition is a single model, my second favorite solution is hill climbing ensemble. Neither of these solutions could win 1st place in Kaggle's April playground competition because the data has too many interactions and too many deep patterns. For Kaggle's April playground competition, we need a large diverse deep RAPIDS cuML stack of 3 levels!

Hill Climbing (linear level 2 model) versus Stacking (non-linear level 2 model)
Hill climbing (or ridge) ensemble generally works well. However in this competition, the dataset was so complicated that a deep stack was the best solution. The most important feature is Episode_Length_minutes. It contains 90%+ of the signal. But it is missing for 11.6% of the data! This means there are two scenarios;

Predict target Listening_Time_minutes with Episode_Length_minutes
Predict Listening_Time_minutes without Episode_Length_minutes
Hill climbing (and ridge) cannot do this (because it uses a linear level 2 model). Imagine that we make one model that does great predicting target with ELM and we build a second model that does great predicting target without ELM. Hill climbing will just take a weighted average of all predictions.

But a stack (non-linear level 2 model) will use predictions from one model when predicting with ELM and use the predictions from another model when predicting without ELM. In other words, instead of taking all predictions from all models, it will take the best predictions from each model (for different situations)!

RAPIDS cuML Stack - 3 Levels of Models!
The secret to building a strong stack is diverse models. (And every model trains with the same 5 KFolds and we must remove all leaks from target encoding, pseudo labeling, etc). Diversity comes from different feature engineering and different models (and/or model hyperparameters).

For each new model I built, I engineered different sets of features using the speed of RAPIDS cuDF. Each model has different customized features that benefit the new model best. And I trained lots of diverse models using the speed of RAPIDS cuML! All models below use the speed of GPU!



Diversity x5
To add diversity to our stack we can take each of the 12 model depicted above and train it in at least 5 different ways described below. Additionally, we can change feature engineering and/or hyperparameters and train more ways. My final stack uses 75 models. So I approximately created each of the above 12 models in 6 different ways!

Every day during the month of April, I spent a few hours and built new diverse models. Using 3xA100 GPU and the speed of RAPIDS cuDF and cuML I would build about a dozen new models (with new complex feature engineering) each day and keep the few models which improved my stack!

(1) Different Sets of Feature Engineering
The typical way to predict Listening_Time_minutes is to train a model using KFold and all columns of train.csv. Additionally we can create more columns with feature engineering. We can build multiple GBDT models each using different engineered features. This provides diversity to our stack. Also we can change GBDT hyperparameters. For example, some times we use max_depth=10 and sometimes we use max_depth=0, max_leaves=1024. These find different interactions and create diverse models. Furthermore, sometimes we can use max_depth=20 to get more interaction and sometimes max_depth=5 for less interaction. Below are 4 other ways to train models in April's playground competition.

(2) Remove Episode_Length_minutes from All Rows!
Based on my discussion here, the feature/column Episode_Length_minutes is important. We can remove Episode_Length_minutes from all rows and train a model to predict Listening_Time_minutes from all other columns. These models will be strong predicting target when Episode_Length_minutes is missing. And the stack will use these models when appropriate.

(3) Predict Ratio of Target divided by Episode_Length_minutes
Based on my discussion here, for each model, we can create a new target with train['new_target'] = train.Listening_Time_minutes / train.Episode_Length_minutes. We can train models to predict this new target. We can then multiply this prediction by Episode_Length_minutes or an imputed value of Episode_Length_minutes from below.

(4) Predict Episode_Length_minutes (use Train.csv and Test.csv)
Based on my discussion here, the feature Episode_Length_minutes is so important, we can train models to predict Episode_Length_minutes. Futhermore, we can use both train.csv and test.csv data to train and predict Episode_Length_minutes. Because both train.csv and test.csv have all the columns necessary.

Afterwards, we can use these ELM predictions in at least 3 ways. (1) We can impute missing values with these ELM preds then train a model. (2) We can replace every row's ELM (both missing and non-missing) with these ELM preds, then train a model. (3) We can multiply these ELM preds by the Ratio preds (from above) to predict the target Listening_Time_minutes. All 3 of these ideas will make new diverse models!

(5) Pseudo Label (use Train.csv and Test.csv)
Based on my discussion here, we see that many columns are important. We can use more information from more columns by using the columns from test.csv. We can add test.csv data with pseudo labels to the training of all our models.

Stacking Models CV Scores
Below are the CV scores for level 1, level 2, and level 3 models (without pseudo labeling). The LB scores are basically the same as the CV scores:

Level 1 Model	Notes	CV Score
RAPIDS cuML Lasso	uses 6000 features!	13.2
RAPIDS cuML SVR	uses 6000 features!	13.2
RAPIDS cuML KNN Regressor	k=51, weight by distance	12.8
RAPIDS cuML Random Forest	max_depth = 32	12.1
NN - MLP	Built by ChatGPT	12.0
NN - TabPFN	20x "SUBSAMPLE_SAMPLES": 10_000	13.2
GBDT - XGBoost	4x models with 4x feature sets	11.8
GBDT - LGBM	diverse from XGBoost	11.8
GBDT - Boost over RAPIDS Lasso	predict Lasso residuals	11.9
GBDT - Boost over RAPIDS SVR	predict SVR residuals	11.9
GBDT - Boost over NN MLP	predict MLP residuals	11.9
AutoML AutoGluon	public notebook here	12.4
---	---	---
Level 2 Model	Notes	CV Score
GBDT XGBoost	uses 73 level 1 models	11.56
NN - MLP	uses 73 level 1 models	11.56
---	---	---
Level 3 Model	Notes	CV Score
Weighted Average	50% / 50%	11.54
.
CREDITS: Thank you @pirhosseinlou for XGBoost single model here and @greysky for LGBM single model here which I used as two of my XGBoost "4x models with 4x feature sets" (and then made a dozen variations of). And thank you @itasps for your AutoML AutoGluon model here. I incorporated all 3 of these public models into my final stack (by re-running with my stack's KFolds and then making a dozen variations of each)!

Final Submission - CV 11.54, Public LB 11.51, Private 11.44, First Place!
My final RAPIDS cuML stack has CV 11.54, Public LB 11.50, Private 11.44, First Place!

Post Comp Analysis
Now that the comp ended, I compare Hill Climb to Stack with my 73x L1 models:

Hill Climbing - CV 11.64 - Public LB 11.57 - Private LB 11.503
Stack - CV 11.54 - Public 11.51 - Private 11.448



①Strong Feature Interaction Exists !
url:https://www.kaggle.com/competitions/playground-series-s5e4/discussion/573002

This playground competition dataset is great! There are lots of interesting patterns! We are not fitting synthetic data artifacts, but rather there are strong patterns between features and target.

We observe lots of interaction between features. An interaction is when one feature affects the relationship between another feature versus target.

Listening_Time_minutes versus Episode_Length_minutes
The strongest correlation in this dataset is Listening_Time_minutes versus Episode_Length_minutes. Overall, the relationship between between Episode_Length_minutes and target Listening_Time_minutes is on average Listening_Time_minutes = 0.728 x Episode_Length_minutes. (Using this relationship alone will achieve RMSE=13.50, wow!). In other words, overall people watch 72.8% of podcasts.



Interaction Features
When other features are present, the overall ratio of 72.8% changes. This is called feature interaction. One feature affects another feature's relationship with target. This is a non-linear relationship that models like linear regression cannot capture (without interaction terms).

Number_of_Ads
When Number_of_Ads = 0,1,2,3 we observe that people watch 78.5, 75.0, 70.8, 67.0% respectively of podcasts. So the feature Number_of_Ads changes the relationship between features Episode_Length_minutes versus Listening_Time_minutes:


Episode_Sentiment
When Episode_Sentiment = Positive, Negative, we observe that people watch 75.0, 71.7% respectively. So the feature Episode_Sentiment changes the relationship between features Episode_Length_minutes versus Listening_Time_minutes:


Genre
When Genre = True Crime or Technology, we observe that people watch 75.0% and when Genre = Business, Comedy, Education, Health, Lifestyle, Music, News, Sports, we observe that people watch 72.0% of podcasts. So the feature Genre changes the relationship between features Episode_Length_minutes versus Listening_Time_minutes:


Conclusion
In conclusion, there are many feature interactions in this dataset. Many features affect other features' relationship with target. Therefore we need to use non-linear models to capture all these patterns (or we can use linear models in creative ensemble/stacking ways) . Furthermore, we need to encourage all our models to learn the many feature interactions that are present in this competition's dataset!



②Direct versus Indirect - Relationship with Target
url:https://www.kaggle.com/competitions/playground-series-s5e4/discussion/574249

From previous posts, we discussed how in general, people watch 72.8% of episodes here.

There are two ways a feature can influence the target Listening_Time_minutes (which approx equals 0.728 x Episode_Length_minutes). A feature can:

Cause people to watch a different percentage (than 0.728) of episode length (direct relationship with target)
Cause episode length to be longer or shorter (indirect relationship with target)
Direct Relationship
In our previous post here, we illustrated features with strong direct relationship with target. Because these features caused users to watch a larger or smaller percentage of Episode_Length_minutes.

Previously we saw that the strongest direct relationship features are Number_of_Ads, Episode_Sentiment, Genre.

Indirect Relationship
Below are each categorical feature's relationship with Episode_Length_minutes. (Note, these plots use original data). If a feature causes Episode_Length_minutes to be longer or shorter, then indirectly that feature will cause the target Listening_Time_minutes to be larger or shorter! (even if it doesn't cause users to watch a different percentage of Episode_Length_minutes).

Now we see that the strongest indirect relationship features are Podcast_Name and Episode_Title! Interesting!


Modeling
Based on the EDA in this discussion post and the EDA in previous two discussion posts here and here, we need to adjust our modeling and solution approaches!

Good Luck, Have Fun!



③Strong Correlation Between Features and Target
url:https://www.kaggle.com/competitions/playground-series-s5e4/discussion/574249

Here is some EDA showing the strong correlation between features and target. There is lots of great signal in this competition! These plots are computed using only the original data but these trends also exist in this competition's synthetic data too:

Numerical Features
We observe a positive linear correlation between Episode_Length_minutes and target. And we see a negative linear correlation between Number_of_Ads and target:


Categorical Features
We observe strong linear correlations between Publication_Day, Publication_Time, and Episode_Sentiment and target:


